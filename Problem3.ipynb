{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Problem3 Final.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ckoFWbwm9-w-"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vietanh239/AI/blob/main/Problem3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6gpBlEhlLgH"
      },
      "source": [
        ""
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfhDIgmtlMI_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d28df1ac-0577-4571-b67f-1dffc3ccf795"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A8FsobQshtj"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch \n",
        "import torch.nn.functional as F\n",
        "import torchtext\n",
        "\n",
        "import requests\n",
        "import tarfile\n",
        "from tqdm import tqdm "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r44Ayu9eXwIS"
      },
      "source": [
        "MODELNAME1 = 'iwslt15-en-vi-rnn.model'\n",
        "MODELNAME2 = 'iwslt15-en-vi-lstm.model'\n",
        "EPOCH = 30\n",
        "BATCHSIZE = 16\n",
        "LR = 0.0001\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHkyfd1b9akh"
      },
      "source": [
        "# PREPROCESSING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckoFWbwm9-w-"
      },
      "source": [
        "## Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD3OUJV1uq7c"
      },
      "source": [
        "url = 'https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/'\n",
        "train_en = [line.split() for line in requests.get(url+'train.en').text.splitlines()]\n",
        "train_vi = [line.split() for line in requests.get(url+'train.vi').text.splitlines()]\n",
        "test_en = [line.split() for line in requests.get(url+'tst2013.en').text.splitlines()]\n",
        "test_vi = [line.split() for line in requests.get(url+'tst2013.vi').text.splitlines()]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3-yzWYtv5-E",
        "outputId": "074536c7-5f49-439c-e3c0-91f5dbc0ddc1"
      },
      "source": [
        "for i in range(10):\n",
        "  print(train_en[i])\n",
        "  print(train_vi[i])\n",
        "print('# of line', len(train_en), len(train_vi), len(test_en), len(test_vi))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Rachel', 'Pike', ':', 'The', 'science', 'behind', 'a', 'climate', 'headline']\n",
            "['Khoa', 'học', 'đằng', 'sau', 'một', 'tiêu', 'đề', 'về', 'khí', 'hậu']\n",
            "['In', '4', 'minutes', ',', 'atmospheric', 'chemist', 'Rachel', 'Pike', 'provides', 'a', 'glimpse', 'of', 'the', 'massive', 'scientific', 'effort', 'behind', 'the', 'bold', 'headlines', 'on', 'climate', 'change', ',', 'with', 'her', 'team', '--', 'one', 'of', 'thousands', 'who', 'contributed', '--', 'taking', 'a', 'risky', 'flight', 'over', 'the', 'rainforest', 'in', 'pursuit', 'of', 'data', 'on', 'a', 'key', 'molecule', '.']\n",
            "['Trong', '4', 'phút', ',', 'chuyên', 'gia', 'hoá', 'học', 'khí', 'quyển', 'Rachel', 'Pike', 'giới', 'thiệu', 'sơ', 'lược', 'về', 'những', 'nỗ', 'lực', 'khoa', 'học', 'miệt', 'mài', 'đằng', 'sau', 'những', 'tiêu', 'đề', 'táo', 'bạo', 'về', 'biến', 'đổi', 'khí', 'hậu', ',', 'cùng', 'với', 'đoàn', 'nghiên', 'cứu', 'của', 'mình', '--', 'hàng', 'ngàn', 'người', 'đã', 'cống', 'hiến', 'cho', 'dự', 'án', 'này', '--', 'một', 'chuyến', 'bay', 'mạo', 'hiểm', 'qua', 'rừng', 'già', 'để', 'tìm', 'kiếm', 'thông', 'tin', 'về', 'một', 'phân', 'tử', 'then', 'chốt', '.']\n",
            "['I', '&apos;d', 'like', 'to', 'talk', 'to', 'you', 'today', 'about', 'the', 'scale', 'of', 'the', 'scientific', 'effort', 'that', 'goes', 'into', 'making', 'the', 'headlines', 'you', 'see', 'in', 'the', 'paper', '.']\n",
            "['Tôi', 'muốn', 'cho', 'các', 'bạn', 'biết', 'về', 'sự', 'to', 'lớn', 'của', 'những', 'nỗ', 'lực', 'khoa', 'học', 'đã', 'góp', 'phần', 'làm', 'nên', 'các', 'dòng', 'tít', 'bạn', 'thường', 'thấy', 'trên', 'báo', '.']\n",
            "['Headlines', 'that', 'look', 'like', 'this', 'when', 'they', 'have', 'to', 'do', 'with', 'climate', 'change', ',', 'and', 'headlines', 'that', 'look', 'like', 'this', 'when', 'they', 'have', 'to', 'do', 'with', 'air', 'quality', 'or', 'smog', '.']\n",
            "['Có', 'những', 'dòng', 'trông', 'như', 'thế', 'này', 'khi', 'bàn', 'về', 'biến', 'đổi', 'khí', 'hậu', ',', 'và', 'như', 'thế', 'này', 'khi', 'nói', 'về', 'chất', 'lượng', 'không', 'khí', 'hay', 'khói', 'bụi', '.']\n",
            "['They', 'are', 'both', 'two', 'branches', 'of', 'the', 'same', 'field', 'of', 'atmospheric', 'science', '.']\n",
            "['Cả', 'hai', 'đều', 'là', 'một', 'nhánh', 'của', 'cùng', 'một', 'lĩnh', 'vực', 'trong', 'ngành', 'khoa', 'học', 'khí', 'quyển', '.']\n",
            "['Recently', 'the', 'headlines', 'looked', 'like', 'this', 'when', 'the', 'Intergovernmental', 'Panel', 'on', 'Climate', 'Change', ',', 'or', 'IPCC', ',', 'put', 'out', 'their', 'report', 'on', 'the', 'state', 'of', 'understanding', 'of', 'the', 'atmospheric', 'system', '.']\n",
            "['Các', 'tiêu', 'đề', 'gần', 'đây', 'trông', 'như', 'thế', 'này', 'khi', 'Ban', 'Điều', 'hành', 'Biến', 'đổi', 'khí', 'hậu', 'Liên', 'chính', 'phủ', ',', 'gọi', 'tắt', 'là', 'IPCC', 'đưa', 'ra', 'bài', 'nghiên', 'cứu', 'của', 'họ', 'về', 'hệ', 'thống', 'khí', 'quyển', '.']\n",
            "['That', 'report', 'was', 'written', 'by', '620', 'scientists', 'from', '40', 'countries', '.']\n",
            "['Nghiên', 'cứu', 'được', 'viết', 'bởi', '620', 'nhà', 'khoa', 'học', 'từ', '40', 'quốc', 'gia', 'khác', 'nhau', '.']\n",
            "['They', 'wrote', 'almost', 'a', 'thousand', 'pages', 'on', 'the', 'topic', '.']\n",
            "['Họ', 'viết', 'gần', '1000', 'trang', 'về', 'chủ', 'đề', 'này', '.']\n",
            "['And', 'all', 'of', 'those', 'pages', 'were', 'reviewed', 'by', 'another', '400-plus', 'scientists', 'and', 'reviewers', ',', 'from', '113', 'countries', '.']\n",
            "['Và', 'tất', 'cả', 'các', 'trang', 'đều', 'được', 'xem', 'xét', 'bởi', '400', 'khoa', 'học', 'gia', 'và', 'nhà', 'phê', 'bình', 'khác', 'từ', '113', 'quốc', 'gia', '.']\n",
            "['It', '&apos;s', 'a', 'big', 'community', '.', 'It', '&apos;s', 'such', 'a', 'big', 'community', ',', 'in', 'fact', ',', 'that', 'our', 'annual', 'gathering', 'is', 'the', 'largest', 'scientific', 'meeting', 'in', 'the', 'world', '.']\n",
            "['Đó', 'là', 'cả', 'một', 'cộng', 'đồng', 'lớn', ',', 'lớn', 'đến', 'nỗi', 'trên', 'thực', 'tế', 'cuộc', 'tụ', 'hội', 'hằng', 'năm', 'của', 'chúng', 'tôi', 'là', 'hội', 'nghị', 'khoa', 'học', '&#91;', 'tự', 'nhiên', '&#93;', 'lớn', 'nhất', 'thế', 'giới', '.']\n",
            "# of line 133317 133317 1268 1268\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP7hrjhA-B0e"
      },
      "source": [
        "## Make Vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13w63o7lxDAy",
        "outputId": "c37969d4-e3e9-4d28-8a70-692f4eb32655"
      },
      "source": [
        "def make_vocab(train_data, min_freq):\n",
        "  vocab = {}\n",
        "  for tokenlist in train_data:\n",
        "    for token in tokenlist:\n",
        "      if token not in vocab:\n",
        "        vocab[token] = 0\n",
        "      vocab[token] += 1 # Đếm các từ data \n",
        "  vocablist = [('<unk>',0),('<pad>', 0), ('<cls>', 0), ('<eos>', 0)] # Danh sách các tuple đặc trưng trước\n",
        "  vocabidx = {} # dict của tuple\n",
        "  for token, freq in vocab.items(): # Từ đó , với số lượng của từ đó trong train data \n",
        "    if freq >= min_freq: # Số lượng từ mà > 3\n",
        "      idx = len(vocablist) # đang là 4 \n",
        "      vocablist.append((token, freq)) #Từ đó , với số lượng của từ đó trong train data \n",
        "      vocabidx[token] = idx  # Từ điển (dict)  \n",
        "  vocabidx['<unk>'] = 0\n",
        "  vocabidx['<pad>'] = 1\n",
        "  vocabidx['<cls>'] = 2\n",
        "  vocabidx['<eos>'] = 3\n",
        "  return vocablist, vocabidx\n",
        "\n",
        "vocablist_en, vocabidx_en = make_vocab(train_en, 3)\n",
        "vocablist_vi, vocabidx_vi = make_vocab(train_vi, 3)\n",
        "\n",
        "print('vocab size en: ', len(vocablist_en))\n",
        "print('vocab size vi: ', len(vocablist_vi))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size en:  24420\n",
            "vocab size vi:  10666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-GCG1mfzBZL",
        "outputId": "e3295ec8-994e-4bf8-c5f4-3d6152b28453"
      },
      "source": [
        "def preprocess(data, vocabidx):\n",
        "  rr = []\n",
        "  for tokenlist in data:\n",
        "    tkl = ['<cls>']\n",
        "    for token in tokenlist:\n",
        "      tkl.append(token if token in vocabidx else '<unk>')\n",
        "    tkl.append('<eos>')\n",
        "    rr.append((tkl))\n",
        "  return rr\n",
        " \n",
        "train_en_prep = preprocess(train_en, vocabidx_en)\n",
        "train_vi_prep = preprocess(train_vi, vocabidx_vi)\n",
        "test_en_prep = preprocess(test_en, vocabidx_en)\n",
        "for i in range(5):\n",
        "  print(train_en_prep[i])\n",
        "  print(train_vi_prep[i])\n",
        "  print(test_en_prep[i])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<cls>', 'Rachel', 'Pike', ':', 'The', 'science', 'behind', 'a', 'climate', 'headline', '<eos>']\n",
            "['<cls>', 'Khoa', 'học', 'đằng', 'sau', 'một', 'tiêu', 'đề', 'về', 'khí', 'hậu', '<eos>']\n",
            "['<cls>', 'When', 'I', 'was', 'little', ',', 'I', 'thought', 'my', 'country', 'was', 'the', 'best', 'on', 'the', 'planet', ',', 'and', 'I', 'grew', 'up', 'singing', 'a', 'song', 'called', '&quot;', 'Nothing', 'To', '<unk>', '.', '&quot;', '<eos>']\n",
            "['<cls>', 'In', '4', 'minutes', ',', 'atmospheric', 'chemist', 'Rachel', 'Pike', 'provides', 'a', 'glimpse', 'of', 'the', 'massive', 'scientific', 'effort', 'behind', 'the', 'bold', 'headlines', 'on', 'climate', 'change', ',', 'with', 'her', 'team', '--', 'one', 'of', 'thousands', 'who', 'contributed', '--', 'taking', 'a', 'risky', 'flight', 'over', 'the', 'rainforest', 'in', 'pursuit', 'of', 'data', 'on', 'a', 'key', 'molecule', '.', '<eos>']\n",
            "['<cls>', 'Trong', '4', 'phút', ',', 'chuyên', 'gia', 'hoá', 'học', 'khí', 'quyển', 'Rachel', 'Pike', 'giới', 'thiệu', 'sơ', 'lược', 'về', 'những', 'nỗ', 'lực', 'khoa', 'học', 'miệt', 'mài', 'đằng', 'sau', 'những', 'tiêu', 'đề', 'táo', 'bạo', 'về', 'biến', 'đổi', 'khí', 'hậu', ',', 'cùng', 'với', 'đoàn', 'nghiên', 'cứu', 'của', 'mình', '--', 'hàng', 'ngàn', 'người', 'đã', 'cống', 'hiến', 'cho', 'dự', 'án', 'này', '--', 'một', 'chuyến', 'bay', 'mạo', 'hiểm', 'qua', 'rừng', 'già', 'để', 'tìm', 'kiếm', 'thông', 'tin', 'về', 'một', 'phân', 'tử', 'then', 'chốt', '.', '<eos>']\n",
            "['<cls>', 'And', 'I', 'was', 'very', 'proud', '.', '<eos>']\n",
            "['<cls>', 'I', '&apos;d', 'like', 'to', 'talk', 'to', 'you', 'today', 'about', 'the', 'scale', 'of', 'the', 'scientific', 'effort', 'that', 'goes', 'into', 'making', 'the', 'headlines', 'you', 'see', 'in', 'the', 'paper', '.', '<eos>']\n",
            "['<cls>', 'Tôi', 'muốn', 'cho', 'các', 'bạn', 'biết', 'về', 'sự', 'to', 'lớn', 'của', 'những', 'nỗ', 'lực', 'khoa', 'học', 'đã', 'góp', 'phần', 'làm', 'nên', 'các', 'dòng', 'tít', 'bạn', 'thường', 'thấy', 'trên', 'báo', '.', '<eos>']\n",
            "['<cls>', 'In', 'school', ',', 'we', 'spent', 'a', 'lot', 'of', 'time', 'studying', 'the', 'history', 'of', 'Kim', '<unk>', ',', 'but', 'we', 'never', 'learned', 'much', 'about', 'the', 'outside', 'world', ',', 'except', 'that', 'America', ',', 'South', 'Korea', ',', 'Japan', 'are', 'the', 'enemies', '.', '<eos>']\n",
            "['<cls>', '<unk>', 'that', 'look', 'like', 'this', 'when', 'they', 'have', 'to', 'do', 'with', 'climate', 'change', ',', 'and', 'headlines', 'that', 'look', 'like', 'this', 'when', 'they', 'have', 'to', 'do', 'with', 'air', 'quality', 'or', 'smog', '.', '<eos>']\n",
            "['<cls>', 'Có', 'những', 'dòng', 'trông', 'như', 'thế', 'này', 'khi', 'bàn', 'về', 'biến', 'đổi', 'khí', 'hậu', ',', 'và', 'như', 'thế', 'này', 'khi', 'nói', 'về', 'chất', 'lượng', 'không', 'khí', 'hay', 'khói', 'bụi', '.', '<eos>']\n",
            "['<cls>', 'Although', 'I', 'often', 'wondered', 'about', 'the', 'outside', 'world', ',', 'I', 'thought', 'I', 'would', 'spend', 'my', 'entire', 'life', 'in', 'North', 'Korea', ',', 'until', 'everything', 'suddenly', 'changed', '.', '<eos>']\n",
            "['<cls>', 'They', 'are', 'both', 'two', 'branches', 'of', 'the', 'same', 'field', 'of', 'atmospheric', 'science', '.', '<eos>']\n",
            "['<cls>', 'Cả', 'hai', 'đều', 'là', 'một', 'nhánh', 'của', 'cùng', 'một', 'lĩnh', 'vực', 'trong', 'ngành', 'khoa', 'học', 'khí', 'quyển', '.', '<eos>']\n",
            "['<cls>', 'When', 'I', 'was', 'seven', 'years', 'old', ',', 'I', 'saw', 'my', 'first', 'public', 'execution', ',', 'but', 'I', 'thought', 'my', 'life', 'in', 'North', 'Korea', 'was', 'normal', '.', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuKn_t_x-EdH"
      },
      "source": [
        "## Make batch & Padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZtKxScuzzJA",
        "outputId": "6770b7e0-ed52-407e-966c-8755237b4d49"
      },
      "source": [
        "train_data = list(zip(train_en_prep, train_vi_prep))\n",
        "train_data.sort(key = lambda x: (len(x[0]), len(x[1])))\n",
        "\n",
        "test_data = list(zip(test_en_prep, test_en, test_vi))\n",
        "\n",
        "for i in range(5):\n",
        "  print(train_data[i])\n",
        "for i in range(5):\n",
        "  print(test_data[i])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(['<cls>', '<eos>'], ['<cls>', '<eos>'])\n",
            "(['<cls>', '<eos>'], ['<cls>', '<eos>'])\n",
            "(['<cls>', '<eos>'], ['<cls>', '<eos>'])\n",
            "(['<cls>', '<eos>'], ['<cls>', '<eos>'])\n",
            "(['<cls>', '<eos>'], ['<cls>', '<eos>'])\n",
            "(['<cls>', 'When', 'I', 'was', 'little', ',', 'I', 'thought', 'my', 'country', 'was', 'the', 'best', 'on', 'the', 'planet', ',', 'and', 'I', 'grew', 'up', 'singing', 'a', 'song', 'called', '&quot;', 'Nothing', 'To', '<unk>', '.', '&quot;', '<eos>'], ['When', 'I', 'was', 'little', ',', 'I', 'thought', 'my', 'country', 'was', 'the', 'best', 'on', 'the', 'planet', ',', 'and', 'I', 'grew', 'up', 'singing', 'a', 'song', 'called', '&quot;', 'Nothing', 'To', 'Envy', '.', '&quot;'], ['Khi', 'tôi', 'còn', 'nhỏ', ',', 'Tôi', 'nghĩ', 'rằng', 'BắcTriều', 'Tiên', 'là', 'đất', 'nước', 'tốt', 'nhất', 'trên', 'thế', 'giới', 'và', 'tôi', 'thường', 'hát', 'bài', '&quot;', 'Chúng', 'ta', 'chẳng', 'có', 'gì', 'phải', 'ghen', 'tị', '.', '&quot;'])\n",
            "(['<cls>', 'And', 'I', 'was', 'very', 'proud', '.', '<eos>'], ['And', 'I', 'was', 'very', 'proud', '.'], ['Tôi', 'đã', 'rất', 'tự', 'hào', 'về', 'đất', 'nước', 'tôi', '.'])\n",
            "(['<cls>', 'In', 'school', ',', 'we', 'spent', 'a', 'lot', 'of', 'time', 'studying', 'the', 'history', 'of', 'Kim', '<unk>', ',', 'but', 'we', 'never', 'learned', 'much', 'about', 'the', 'outside', 'world', ',', 'except', 'that', 'America', ',', 'South', 'Korea', ',', 'Japan', 'are', 'the', 'enemies', '.', '<eos>'], ['In', 'school', ',', 'we', 'spent', 'a', 'lot', 'of', 'time', 'studying', 'the', 'history', 'of', 'Kim', 'Il-Sung', ',', 'but', 'we', 'never', 'learned', 'much', 'about', 'the', 'outside', 'world', ',', 'except', 'that', 'America', ',', 'South', 'Korea', ',', 'Japan', 'are', 'the', 'enemies', '.'], ['Ở', 'trường', ',', 'chúng', 'tôi', 'dành', 'rất', 'nhiều', 'thời', 'gian', 'để', 'học', 'về', 'cuộc', 'đời', 'của', 'chủ', 'tịch', 'Kim', 'II-', 'Sung', ',', 'nhưng', 'lại', 'không', 'học', 'nhiều', 'về', 'thế', 'giới', 'bên', 'ngoài', ',', 'ngoại', 'trừ', 'việc', 'Hoa', 'Kỳ', ',', 'Hàn', 'Quốc', 'và', 'Nhật', 'Bản', 'là', 'kẻ', 'thù', 'của', 'chúng', 'tôi', '.'])\n",
            "(['<cls>', 'Although', 'I', 'often', 'wondered', 'about', 'the', 'outside', 'world', ',', 'I', 'thought', 'I', 'would', 'spend', 'my', 'entire', 'life', 'in', 'North', 'Korea', ',', 'until', 'everything', 'suddenly', 'changed', '.', '<eos>'], ['Although', 'I', 'often', 'wondered', 'about', 'the', 'outside', 'world', ',', 'I', 'thought', 'I', 'would', 'spend', 'my', 'entire', 'life', 'in', 'North', 'Korea', ',', 'until', 'everything', 'suddenly', 'changed', '.'], ['Mặc', 'dù', 'tôi', 'đã', 'từng', 'tự', 'hỏi', 'không', 'biết', 'thế', 'giới', 'bên', 'ngoài', 'kia', 'như', 'thế', 'nào', ',', 'nhưng', 'tôi', 'vẫn', 'nghĩ', 'rằng', 'mình', 'sẽ', 'sống', 'cả', 'cuộc', 'đời', 'ở', 'BắcTriều', 'Tiên', ',', 'cho', 'tới', 'khi', 'tất', 'cả', 'mọi', 'thứ', 'đột', 'nhiên', 'thay', 'đổi', '.'])\n",
            "(['<cls>', 'When', 'I', 'was', 'seven', 'years', 'old', ',', 'I', 'saw', 'my', 'first', 'public', 'execution', ',', 'but', 'I', 'thought', 'my', 'life', 'in', 'North', 'Korea', 'was', 'normal', '.', '<eos>'], ['When', 'I', 'was', 'seven', 'years', 'old', ',', 'I', 'saw', 'my', 'first', 'public', 'execution', ',', 'but', 'I', 'thought', 'my', 'life', 'in', 'North', 'Korea', 'was', 'normal', '.'], ['Khi', 'tôi', 'lên', '7', ',', 'tôi', 'chứng', 'kiến', 'cảnh', 'người', 'ta', 'xử', 'bắn', 'công', 'khai', 'lần', 'đầu', 'tiên', 'trong', 'đời', ',', 'nhưng', 'tôi', 'vẫn', 'nghĩ', 'cuộc', 'sống', 'của', 'mình', 'ở', 'đây', 'là', 'hoàn', 'toàn', 'bình', 'thường', '.'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cihKDz3l1aGC",
        "outputId": "53a2ea0d-c46a-4fe6-f67a-c28fc358c065"
      },
      "source": [
        "def make_batch(data, batchsize):\n",
        "  bb = []\n",
        "  ben = []\n",
        "  bvi = []\n",
        "  for en,vi in data:\n",
        "    ben.append(en)\n",
        "    bvi.append(vi)\n",
        "    if len(ben) >= batchsize:\n",
        "      bb.append((ben, bvi))\n",
        "      ben = []\n",
        "      bvi = []\n",
        "  if len(ben) > 0:\n",
        "    bb.append((ben, bvi))\n",
        "  return bb\n",
        "\n",
        "train_data = make_batch(train_data, BATCHSIZE)\n",
        "\n",
        "for i in range(2):\n",
        "  print(train_data[i])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "([['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']], [['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']])\n",
            "([['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']], [['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PT3mMydz2Jee",
        "outputId": "4b63b471-2e54-419e-fdaa-41177ddae4d2"
      },
      "source": [
        "def padding_batch(b):\n",
        "  maxlen = max([len(x) for x in b])\n",
        "  for tkl in b:\n",
        "    for i in range(maxlen - len(tkl)):\n",
        "      tkl.append('<pad>')\n",
        "\n",
        "def padding(bb):\n",
        "  for ben, bvi in bb:\n",
        "    padding_batch(ben)\n",
        "    padding_batch(bvi)\n",
        "\n",
        "padding(train_data)\n",
        "\n",
        "for i in range(3):\n",
        "  print(train_data[i])\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "([['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']], [['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']])\n",
            "([['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']], [['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']])\n",
            "([['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']], [['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXta_9yt21KC",
        "outputId": "8b31eaf1-7782-4df9-967b-8704b9c40b65"
      },
      "source": [
        "train_data = [([[vocabidx_en[token] for token in tokenlist] for tokenlist in ben],\n",
        "               [[vocabidx_vi[token] for token in tokenlist] for tokenlist in bvi]) for ben, bvi in train_data]\n",
        "test_data = [([vocabidx_en[token] for token in enprep], en, vi) for enprep, en, vi in test_data]\n",
        "for i in range(3):\n",
        "  print(train_data[i])\n",
        "  print(test_data[i])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "([[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]], [[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]])\n",
            "([2, 444, 49, 96, 678, 16, 49, 884, 429, 823, 96, 22, 1203, 28, 22, 203, 16, 70, 49, 722, 218, 2403, 10, 2271, 178, 545, 9225, 868, 0, 48, 545, 3], ['When', 'I', 'was', 'little', ',', 'I', 'thought', 'my', 'country', 'was', 'the', 'best', 'on', 'the', 'planet', ',', 'and', 'I', 'grew', 'up', 'singing', 'a', 'song', 'called', '&quot;', 'Nothing', 'To', 'Envy', '.', '&quot;'], ['Khi', 'tôi', 'còn', 'nhỏ', ',', 'Tôi', 'nghĩ', 'rằng', 'BắcTriều', 'Tiên', 'là', 'đất', 'nước', 'tốt', 'nhất', 'trên', 'thế', 'giới', 'và', 'tôi', 'thường', 'hát', 'bài', '&quot;', 'Chúng', 'ta', 'chẳng', 'có', 'gì', 'phải', 'ghen', 'tị', '.', '&quot;'])\n",
            "([[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]], [[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]])\n",
            "([2, 109, 49, 96, 198, 6154, 48, 3], ['And', 'I', 'was', 'very', 'proud', '.'], ['Tôi', 'đã', 'rất', 'tự', 'hào', 'về', 'đất', 'nước', 'tôi', '.'])\n",
            "([[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]], [[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]])\n",
            "([2, 13, 702, 16, 171, 1625, 10, 1318, 21, 365, 1830, 22, 625, 21, 8919, 0, 16, 442, 171, 186, 1834, 210, 56, 22, 1737, 128, 16, 1741, 58, 1317, 16, 1258, 1259, 16, 1512, 76, 22, 2722, 48, 3], ['In', 'school', ',', 'we', 'spent', 'a', 'lot', 'of', 'time', 'studying', 'the', 'history', 'of', 'Kim', 'Il-Sung', ',', 'but', 'we', 'never', 'learned', 'much', 'about', 'the', 'outside', 'world', ',', 'except', 'that', 'America', ',', 'South', 'Korea', ',', 'Japan', 'are', 'the', 'enemies', '.'], ['Ở', 'trường', ',', 'chúng', 'tôi', 'dành', 'rất', 'nhiều', 'thời', 'gian', 'để', 'học', 'về', 'cuộc', 'đời', 'của', 'chủ', 'tịch', 'Kim', 'II-', 'Sung', ',', 'nhưng', 'lại', 'không', 'học', 'nhiều', 'về', 'thế', 'giới', 'bên', 'ngoài', ',', 'ngoại', 'trừ', 'việc', 'Hoa', 'Kỳ', ',', 'Hàn', 'Quốc', 'và', 'Nhật', 'Bản', 'là', 'kẻ', 'thù', 'của', 'chúng', 'tôi', '.'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me6uuDtmFKjo"
      },
      "source": [
        "# Transfomer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ptiIyWaKpnj"
      },
      "source": [
        "from torch import Tensor\n",
        "from torch import nn\n",
        "from torch.nn import Transformer\n",
        "import math"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDrzCg24Km8w"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 900):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Seq2Seq Network\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.05):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size,\n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)\n",
        "        \n",
        "    # def evaluate(self):\n",
        "\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWJmnaJfPer2"
      },
      "source": [
        "import time\n",
        "torch.manual_seed(0)\n",
        "\n",
        "MODELNAME = 'transfomers.model'\n",
        "SRC_VOCAB_SIZE = len(vocablist_en)\n",
        "TGT_VOCAB_SIZE = len(vocablist_vi)\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "\n",
        "model = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "model = model.to(DEVICE)\n",
        "PAD_IDX= 1\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "\n",
        "def train():\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "  model.train()\n",
        "  all_time= 0\n",
        "  for epoch in range(30):\n",
        "    start= time.time()\n",
        "    loss = 0\n",
        "\n",
        "    for en, vi in train_data:\n",
        "      \n",
        "      en = torch.tensor(en, dtype=torch.int64).transpose(0,1).to(DEVICE)\n",
        "      vi = torch.tensor(vi, dtype=torch.int64).transpose(0,1).to(DEVICE)\n",
        "      tgt_input = vi[:-1, :]\n",
        "      \n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(en, tgt_input)\n",
        "      y = model(en, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "      tgt_out = vi[1:, :]\n",
        "      batchloss = loss_fn(y.reshape(-1, y.shape[-1]), tgt_out.reshape(-1))\n",
        "      batchloss.backward()\n",
        "      optimizer.step()\n",
        "      loss = loss + batchloss.item()\n",
        "    end= time.time()\n",
        "    all_time+=(end-start)\n",
        "    print(\"epoch\", epoch, \": loss\", loss, \"Epoch time\",end-start)\n",
        "    if epoch %5 ==0:\n",
        "      print(\"Saving model at epoch: {}\".format(epoch))\n",
        "      torch.save(model.state_dict(), \"/content/drive/MyDrive/ArtificialIntelligence/Problem3/model/transform{}\".format(epoch))\n",
        "  torch.save(model.state_dict(), MODELNAME)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fw2P3EH0L0T9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02cade01-a7ee-408c-ff03-b1911089d953"
      },
      "source": [
        "!pip install prettytable\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "def count_parameters(model):\n",
        "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad: continue\n",
        "        param = parameter.numel()\n",
        "        table.add_row([name, param])\n",
        "        total_params+=param\n",
        "    print(table)\n",
        "    print(f\"Total Trainable Params: {total_params}\")\n",
        "    return total_params\n",
        "    \n",
        "count_parameters(model)\n",
        "\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from prettytable) (4.5.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prettytable) (0.2.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->prettytable) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->prettytable) (3.7.4.3)\n",
            "+-------------------------------------------------------------+------------+\n",
            "|                           Modules                           | Parameters |\n",
            "+-------------------------------------------------------------+------------+\n",
            "|    transformer.encoder.layers.0.self_attn.in_proj_weight    |   786432   |\n",
            "|     transformer.encoder.layers.0.self_attn.in_proj_bias     |    1536    |\n",
            "|    transformer.encoder.layers.0.self_attn.out_proj.weight   |   262144   |\n",
            "|     transformer.encoder.layers.0.self_attn.out_proj.bias    |    512     |\n",
            "|         transformer.encoder.layers.0.linear1.weight         |   262144   |\n",
            "|          transformer.encoder.layers.0.linear1.bias          |    512     |\n",
            "|         transformer.encoder.layers.0.linear2.weight         |   262144   |\n",
            "|          transformer.encoder.layers.0.linear2.bias          |    512     |\n",
            "|          transformer.encoder.layers.0.norm1.weight          |    512     |\n",
            "|           transformer.encoder.layers.0.norm1.bias           |    512     |\n",
            "|          transformer.encoder.layers.0.norm2.weight          |    512     |\n",
            "|           transformer.encoder.layers.0.norm2.bias           |    512     |\n",
            "|    transformer.encoder.layers.1.self_attn.in_proj_weight    |   786432   |\n",
            "|     transformer.encoder.layers.1.self_attn.in_proj_bias     |    1536    |\n",
            "|    transformer.encoder.layers.1.self_attn.out_proj.weight   |   262144   |\n",
            "|     transformer.encoder.layers.1.self_attn.out_proj.bias    |    512     |\n",
            "|         transformer.encoder.layers.1.linear1.weight         |   262144   |\n",
            "|          transformer.encoder.layers.1.linear1.bias          |    512     |\n",
            "|         transformer.encoder.layers.1.linear2.weight         |   262144   |\n",
            "|          transformer.encoder.layers.1.linear2.bias          |    512     |\n",
            "|          transformer.encoder.layers.1.norm1.weight          |    512     |\n",
            "|           transformer.encoder.layers.1.norm1.bias           |    512     |\n",
            "|          transformer.encoder.layers.1.norm2.weight          |    512     |\n",
            "|           transformer.encoder.layers.1.norm2.bias           |    512     |\n",
            "|    transformer.encoder.layers.2.self_attn.in_proj_weight    |   786432   |\n",
            "|     transformer.encoder.layers.2.self_attn.in_proj_bias     |    1536    |\n",
            "|    transformer.encoder.layers.2.self_attn.out_proj.weight   |   262144   |\n",
            "|     transformer.encoder.layers.2.self_attn.out_proj.bias    |    512     |\n",
            "|         transformer.encoder.layers.2.linear1.weight         |   262144   |\n",
            "|          transformer.encoder.layers.2.linear1.bias          |    512     |\n",
            "|         transformer.encoder.layers.2.linear2.weight         |   262144   |\n",
            "|          transformer.encoder.layers.2.linear2.bias          |    512     |\n",
            "|          transformer.encoder.layers.2.norm1.weight          |    512     |\n",
            "|           transformer.encoder.layers.2.norm1.bias           |    512     |\n",
            "|          transformer.encoder.layers.2.norm2.weight          |    512     |\n",
            "|           transformer.encoder.layers.2.norm2.bias           |    512     |\n",
            "|               transformer.encoder.norm.weight               |    512     |\n",
            "|                transformer.encoder.norm.bias                |    512     |\n",
            "|    transformer.decoder.layers.0.self_attn.in_proj_weight    |   786432   |\n",
            "|     transformer.decoder.layers.0.self_attn.in_proj_bias     |    1536    |\n",
            "|    transformer.decoder.layers.0.self_attn.out_proj.weight   |   262144   |\n",
            "|     transformer.decoder.layers.0.self_attn.out_proj.bias    |    512     |\n",
            "|  transformer.decoder.layers.0.multihead_attn.in_proj_weight |   786432   |\n",
            "|   transformer.decoder.layers.0.multihead_attn.in_proj_bias  |    1536    |\n",
            "| transformer.decoder.layers.0.multihead_attn.out_proj.weight |   262144   |\n",
            "|  transformer.decoder.layers.0.multihead_attn.out_proj.bias  |    512     |\n",
            "|         transformer.decoder.layers.0.linear1.weight         |   262144   |\n",
            "|          transformer.decoder.layers.0.linear1.bias          |    512     |\n",
            "|         transformer.decoder.layers.0.linear2.weight         |   262144   |\n",
            "|          transformer.decoder.layers.0.linear2.bias          |    512     |\n",
            "|          transformer.decoder.layers.0.norm1.weight          |    512     |\n",
            "|           transformer.decoder.layers.0.norm1.bias           |    512     |\n",
            "|          transformer.decoder.layers.0.norm2.weight          |    512     |\n",
            "|           transformer.decoder.layers.0.norm2.bias           |    512     |\n",
            "|          transformer.decoder.layers.0.norm3.weight          |    512     |\n",
            "|           transformer.decoder.layers.0.norm3.bias           |    512     |\n",
            "|    transformer.decoder.layers.1.self_attn.in_proj_weight    |   786432   |\n",
            "|     transformer.decoder.layers.1.self_attn.in_proj_bias     |    1536    |\n",
            "|    transformer.decoder.layers.1.self_attn.out_proj.weight   |   262144   |\n",
            "|     transformer.decoder.layers.1.self_attn.out_proj.bias    |    512     |\n",
            "|  transformer.decoder.layers.1.multihead_attn.in_proj_weight |   786432   |\n",
            "|   transformer.decoder.layers.1.multihead_attn.in_proj_bias  |    1536    |\n",
            "| transformer.decoder.layers.1.multihead_attn.out_proj.weight |   262144   |\n",
            "|  transformer.decoder.layers.1.multihead_attn.out_proj.bias  |    512     |\n",
            "|         transformer.decoder.layers.1.linear1.weight         |   262144   |\n",
            "|          transformer.decoder.layers.1.linear1.bias          |    512     |\n",
            "|         transformer.decoder.layers.1.linear2.weight         |   262144   |\n",
            "|          transformer.decoder.layers.1.linear2.bias          |    512     |\n",
            "|          transformer.decoder.layers.1.norm1.weight          |    512     |\n",
            "|           transformer.decoder.layers.1.norm1.bias           |    512     |\n",
            "|          transformer.decoder.layers.1.norm2.weight          |    512     |\n",
            "|           transformer.decoder.layers.1.norm2.bias           |    512     |\n",
            "|          transformer.decoder.layers.1.norm3.weight          |    512     |\n",
            "|           transformer.decoder.layers.1.norm3.bias           |    512     |\n",
            "|    transformer.decoder.layers.2.self_attn.in_proj_weight    |   786432   |\n",
            "|     transformer.decoder.layers.2.self_attn.in_proj_bias     |    1536    |\n",
            "|    transformer.decoder.layers.2.self_attn.out_proj.weight   |   262144   |\n",
            "|     transformer.decoder.layers.2.self_attn.out_proj.bias    |    512     |\n",
            "|  transformer.decoder.layers.2.multihead_attn.in_proj_weight |   786432   |\n",
            "|   transformer.decoder.layers.2.multihead_attn.in_proj_bias  |    1536    |\n",
            "| transformer.decoder.layers.2.multihead_attn.out_proj.weight |   262144   |\n",
            "|  transformer.decoder.layers.2.multihead_attn.out_proj.bias  |    512     |\n",
            "|         transformer.decoder.layers.2.linear1.weight         |   262144   |\n",
            "|          transformer.decoder.layers.2.linear1.bias          |    512     |\n",
            "|         transformer.decoder.layers.2.linear2.weight         |   262144   |\n",
            "|          transformer.decoder.layers.2.linear2.bias          |    512     |\n",
            "|          transformer.decoder.layers.2.norm1.weight          |    512     |\n",
            "|           transformer.decoder.layers.2.norm1.bias           |    512     |\n",
            "|          transformer.decoder.layers.2.norm2.weight          |    512     |\n",
            "|           transformer.decoder.layers.2.norm2.bias           |    512     |\n",
            "|          transformer.decoder.layers.2.norm3.weight          |    512     |\n",
            "|           transformer.decoder.layers.2.norm3.bias           |    512     |\n",
            "|               transformer.decoder.norm.weight               |    512     |\n",
            "|                transformer.decoder.norm.bias                |    512     |\n",
            "|                       generator.weight                      |  5460992   |\n",
            "|                        generator.bias                       |   10666    |\n",
            "|                 src_tok_emb.embedding.weight                |  12503040  |\n",
            "|                 tgt_tok_emb.embedding.weight                |  5460992   |\n",
            "+-------------------------------------------------------------+------------+\n",
            "Total Trainable Params: 36060586\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36060586"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lYGEB1XRxSU",
        "outputId": "4cafc74e-30bd-40b0-8235-6b33ad5a2585"
      },
      "source": [
        "train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0 : loss 33720.518535614014 Epoch time 293.92053508758545\n",
            "Saving model at epoch: 0\n",
            "epoch 1 : loss 23736.11585680209 Epoch time 294.74175906181335\n",
            "epoch 2 : loss 20569.27555437293 Epoch time 294.54462599754333\n",
            "epoch 3 : loss 18757.547611678485 Epoch time 294.5156626701355\n",
            "epoch 4 : loss 17556.006887139054 Epoch time 294.136691570282\n",
            "epoch 5 : loss 16687.58778662607 Epoch time 294.71885871887207\n",
            "Saving model at epoch: 5\n",
            "epoch 6 : loss 16002.004792058142 Epoch time 296.0768229961395\n",
            "epoch 7 : loss 15467.618284447817 Epoch time 295.4152431488037\n",
            "epoch 8 : loss 15020.654229484266 Epoch time 293.664297580719\n",
            "epoch 9 : loss 14637.839362376137 Epoch time 293.97996640205383\n",
            "epoch 10 : loss 14304.080040644854 Epoch time 294.0170736312866\n",
            "Saving model at epoch: 10\n",
            "epoch 11 : loss 13973.43175695138 Epoch time 294.167950630188\n",
            "epoch 12 : loss 13676.93149445206 Epoch time 293.94838643074036\n",
            "epoch 13 : loss 13392.635522043798 Epoch time 293.8659884929657\n",
            "epoch 14 : loss 13125.618030989543 Epoch time 293.57768273353577\n",
            "epoch 15 : loss 12859.511181048583 Epoch time 293.30866718292236\n",
            "Saving model at epoch: 15\n",
            "epoch 16 : loss 12602.692948128097 Epoch time 294.2055096626282\n",
            "epoch 17 : loss 12357.13078173669 Epoch time 293.99881386756897\n",
            "epoch 18 : loss 12128.033639695495 Epoch time 294.4969184398651\n",
            "epoch 19 : loss 11901.891580482945 Epoch time 294.3457715511322\n",
            "epoch 20 : loss 11680.220777714625 Epoch time 294.24514293670654\n",
            "Saving model at epoch: 20\n",
            "epoch 21 : loss 11473.599619154818 Epoch time 294.07997941970825\n",
            "epoch 22 : loss 11265.308392568491 Epoch time 293.5336220264435\n",
            "epoch 23 : loss 11077.074638781138 Epoch time 293.4634394645691\n",
            "epoch 24 : loss 10889.236279997975 Epoch time 293.2737536430359\n",
            "epoch 25 : loss 10711.053773147054 Epoch time 293.9254539012909\n",
            "Saving model at epoch: 25\n",
            "epoch 26 : loss 10539.124669807032 Epoch time 295.6241285800934\n",
            "epoch 27 : loss 10378.23069746932 Epoch time 293.6816942691803\n",
            "epoch 28 : loss 10205.661263205111 Epoch time 294.895804643631\n",
            "epoch 29 : loss 10052.346876776312 Epoch time 294.10831332206726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_iFmbM8SrbP"
      },
      "source": [
        " torch.save(model.state_dict(), \"/content/drive/MyDrive/ArtificialIntelligence/Problem3/model/transform30\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_m-tN_QMoiX"
      },
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == vocabidx_en['<pad>']).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == vocabidx_vi['<pad>']).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHbT6K8TUQud"
      },
      "source": [
        "#Case 30 epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nB5fii1ISD6"
      },
      "source": [
        "def evaluate(model, src, max_len, start_symbol):\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = torch.zeros((num_tokens, num_tokens),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    pred = []\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0)).type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        # next_word = out.squeeze().argmax()\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        # print(prob)\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == vocabidx_en['<eos>']:\n",
        "            break\n",
        "        pred_y = vocablist_vi[next_word][0]\n",
        "        pred.append(pred_y)\n",
        "    return pred\n",
        "\n",
        "def test():\n",
        "  model = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM).to(DEVICE)\n",
        "  model.load_state_dict(torch.load(MODELNAME))\n",
        "  model.eval()\n",
        "  ref = []\n",
        "  pred = []\n",
        "  for enprep, en, vi in test_data:\n",
        "    input = torch.tensor([enprep], dtype=torch.int64).transpose(0,1).to(DEVICE).view(-1, 1)\n",
        "    p = evaluate(model, input, 50, vocabidx_en['<cls>'])\n",
        "    ref.append([vi])\n",
        "    pred.append(p)\n",
        "  bleu = torchtext.data.metrics.bleu_score(pred, ref)\n",
        "  print(\"total: \", len(test_data))\n",
        "  print(\"bleu: \", bleu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw_Xk8aB80oG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10aeed99-3234-455f-ce9a-9995358a7a94"
      },
      "source": [
        "test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total:  1268\n",
            "bleu:  0.19087384641170502\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hjhY_dJUT0N"
      },
      "source": [
        "#Case 20 epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODdZPe_d81D2"
      },
      "source": [
        "\n",
        "def test20():\n",
        "  model = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM).to(DEVICE)\n",
        "  model.load_state_dict(torch.load(\"/content/drive/MyDrive/ArtificialIntelligence/Problem3/model/transform20\"))\n",
        "  model.eval()\n",
        "  ref = []\n",
        "  pred = []\n",
        "  for enprep, en, vi in test_data:\n",
        "    input = torch.tensor([enprep], dtype=torch.int64).transpose(0,1).to(DEVICE).view(-1, 1)\n",
        "    p = evaluate(model, input, 50, vocabidx_en['<cls>'])\n",
        "    ref.append([vi])\n",
        "    pred.append(p)\n",
        "  bleu = torchtext.data.metrics.bleu_score(pred, ref)\n",
        "  print(\"total: \", len(test_data))\n",
        "  print(\"bleu: \", bleu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIkmjHWCUcpF",
        "outputId": "343cd9c3-eba1-41d4-dec2-7b0b83b1660d"
      },
      "source": [
        "test20()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total:  1268\n",
            "bleu:  0.17848320305347443\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdbcKbTTUsa2"
      },
      "source": [
        "#Case 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mtij1egUdVH"
      },
      "source": [
        "def test10():\n",
        "  model = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM).to(DEVICE)\n",
        "  model.load_state_dict(torch.load(\"/content/drive/MyDrive/ArtificialIntelligence/Problem3/model/transform10\"))\n",
        "  model.eval()\n",
        "  ref = []\n",
        "  pred = []\n",
        "  for enprep, en, vi in test_data:\n",
        "    input = torch.tensor([enprep], dtype=torch.int64).transpose(0,1).to(DEVICE).view(-1, 1)\n",
        "    p = evaluate(model, input, 50, vocabidx_en['<cls>'])\n",
        "    ref.append([vi])\n",
        "    pred.append(p)\n",
        "  bleu = torchtext.data.metrics.bleu_score(pred, ref)\n",
        "  print(\"total: \", len(test_data))\n",
        "  print(\"bleu: \", bleu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-aqXdOFVcca",
        "outputId": "3fdc34ba-ac80-4134-9615-5ec454dd70c0"
      },
      "source": [
        "test10()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total:  1268\n",
            "bleu:  0.1869015246629715\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-5dy5UOWnIl"
      },
      "source": [
        "#Summary model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d-F2k4NWphd",
        "outputId": "0d8eef30-4d00-4951-8af4-eedc14e7ac84"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seq2SeqTransformer(\n",
            "  (transformer): Transformer(\n",
            "    (encoder): TransformerEncoder(\n",
            "      (layers): ModuleList(\n",
            "        (0): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.05, inplace=False)\n",
            "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.05, inplace=False)\n",
            "          (dropout2): Dropout(p=0.05, inplace=False)\n",
            "        )\n",
            "        (1): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.05, inplace=False)\n",
            "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.05, inplace=False)\n",
            "          (dropout2): Dropout(p=0.05, inplace=False)\n",
            "        )\n",
            "        (2): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.05, inplace=False)\n",
            "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.05, inplace=False)\n",
            "          (dropout2): Dropout(p=0.05, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (decoder): TransformerDecoder(\n",
            "      (layers): ModuleList(\n",
            "        (0): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.05, inplace=False)\n",
            "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.05, inplace=False)\n",
            "          (dropout2): Dropout(p=0.05, inplace=False)\n",
            "          (dropout3): Dropout(p=0.05, inplace=False)\n",
            "        )\n",
            "        (1): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.05, inplace=False)\n",
            "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.05, inplace=False)\n",
            "          (dropout2): Dropout(p=0.05, inplace=False)\n",
            "          (dropout3): Dropout(p=0.05, inplace=False)\n",
            "        )\n",
            "        (2): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.05, inplace=False)\n",
            "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.05, inplace=False)\n",
            "          (dropout2): Dropout(p=0.05, inplace=False)\n",
            "          (dropout3): Dropout(p=0.05, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (generator): Linear(in_features=512, out_features=10666, bias=True)\n",
            "  (src_tok_emb): TokenEmbedding(\n",
            "    (embedding): Embedding(24420, 512)\n",
            "  )\n",
            "  (tgt_tok_emb): TokenEmbedding(\n",
            "    (embedding): Embedding(10666, 512)\n",
            "  )\n",
            "  (positional_encoding): PositionalEncoding(\n",
            "    (dropout): Dropout(p=0.05, inplace=False)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkRwHxEkWqof"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}